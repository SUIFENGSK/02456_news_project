{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1+cu124\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "BLACKHOLE = False\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" # fixes problem with graph\n",
    "import torch\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "\n",
    "# Check gpu availability\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Test:\n",
    "#print(torch.zeros(1).cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSModel(\n",
      "  (news_encoder): NewsEncoder(\n",
      "    (embedding): Embedding(1000, 100)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (self_attention): SelfAttention(\n",
      "      (query_proj): Linear(in_features=100, out_features=256, bias=True)\n",
      "      (key_proj): Linear(in_features=100, out_features=256, bias=True)\n",
      "      (value_proj): Linear(in_features=100, out_features=256, bias=True)\n",
      "    )\n",
      "    (dense_layers): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=200, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "      (4): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "      (7): Dropout(p=0.2, inplace=False)\n",
      "      (8): Linear(in_features=200, out_features=256, bias=True)\n",
      "      (9): ReLU()\n",
      "      (10): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (11): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (att_layer): AttLayer2(\n",
      "      (attention_projection): Linear(in_features=256, out_features=1000, bias=True)\n",
      "      (query_vector): Linear(in_features=1000, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (user_encoder): UserEncoder(\n",
      "    (news_encoder): NewsEncoder(\n",
      "      (embedding): Embedding(1000, 100)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (self_attention): SelfAttention(\n",
      "        (query_proj): Linear(in_features=100, out_features=256, bias=True)\n",
      "        (key_proj): Linear(in_features=100, out_features=256, bias=True)\n",
      "        (value_proj): Linear(in_features=100, out_features=256, bias=True)\n",
      "      )\n",
      "      (dense_layers): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=200, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "        (4): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (5): ReLU()\n",
      "        (6): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (7): Dropout(p=0.2, inplace=False)\n",
      "        (8): Linear(in_features=200, out_features=256, bias=True)\n",
      "        (9): ReLU()\n",
      "        (10): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (11): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (att_layer): AttLayer2(\n",
      "        (attention_projection): Linear(in_features=256, out_features=1000, bias=True)\n",
      "        (query_vector): Linear(in_features=1000, out_features=1, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (self_attention): SelfAttention(\n",
      "      (query_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (key_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (att_layer): AttLayer2(\n",
      "      (attention_projection): Linear(in_features=256, out_features=1000, bias=True)\n",
      "      (query_vector): Linear(in_features=1000, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (click_predictor): ClickPredictor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from nrms import NRMSModel\n",
    "from hyperparameters import hparams_nrms\n",
    "import numpy as np\n",
    "\n",
    "hparams = hparams_nrms()\n",
    "\n",
    "MAX_TITLE_LENGTH = 10\n",
    "HISTORY_SIZE = 30\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "word2vec_embedding = np.random.rand(1000, 100)\n",
    "\n",
    "# PARAMETERS\n",
    "hparams.title_size = MAX_TITLE_LENGTH\n",
    "hparams.history_size = HISTORY_SIZE\n",
    "hparams.batch_size = BATCH_SIZE\n",
    "hparams.candidate_size = 5\n",
    "\n",
    "# MODEL ARCHITECTURE\n",
    "hparams.head_num = 16\n",
    "hparams.head_dim = 16\n",
    "hparams.attention_hidden_dim = 1000\n",
    "hparams.linear_hidden_dim = 200\n",
    "hparams.embedding_dim = word2vec_embedding.shape[1]\n",
    "\n",
    "hparams.use_positional_encoding = False\n",
    "hparams.use_learned_positions = False\n",
    "\n",
    "# MODEL OPTIMIZER:\n",
    "hparams.optimizer = \"adam\"\n",
    "hparams.loss = \"cross_entropy_loss\"\n",
    "hparams.dropout = 0.2\n",
    "hparams.learning_rate = 1e-4\n",
    "\n",
    "model = NRMSModel(hparams=hparams, word2vec_embedding=word2vec_embedding)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "if hparams.loss == \"cross_entropy_loss\":\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "elif hparams.loss == \"mse_loss\":\n",
    "    criterion = nn.MSELoss()\n",
    "else:\n",
    "    raise ValueError(f\"Loss function {hparams.loss} not supported\")\n",
    "\n",
    "if hparams.optimizer == \"adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hparams_nrms.learning_rate)\n",
    "else:\n",
    "    raise ValueError(f\"Optimizer {hparams.optimizer} not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "10\n",
      "4\n",
      "1000\n",
      "(64, 30, 10)\n",
      "(64, 5, 10)\n",
      "(64, 5)\n",
      "Batch 0\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([2.6596e-03, 4.2923e-03, 9.9304e-01, 7.2532e-06, 3.5315e-06],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0., 0.], device='cuda:0')\n",
      "Batch 1\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([9.6764e-01, 2.0515e-03, 2.8264e-02, 1.8514e-03, 1.9588e-04],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0., 0.], device='cuda:0')\n",
      "Batch 2\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([0.0083, 0.0334, 0.0035, 0.8053, 0.1494], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0., 0.], device='cuda:0')\n",
      "Batch 3\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([3.3832e-03, 5.9570e-03, 2.6699e-01, 5.6589e-04, 7.2310e-01],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 1., 0.], device='cuda:0')\n",
      "Batch 4\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([0.1514, 0.0650, 0.0023, 0.7778, 0.0035], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0., 0.], device='cuda:0')\n",
      "Batch 5\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([7.3667e-04, 9.2207e-01, 1.8985e-07, 1.6716e-03, 7.5523e-02],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0., 0., 0.], device='cuda:0')\n",
      "Batch 6\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.8785e-04, 5.5189e-03, 3.1740e-01, 5.4362e-01, 1.3328e-01],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0., 0., 0.], device='cuda:0')\n",
      "Batch 7\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.3447e-03, 9.8324e-01, 1.4519e-03, 1.3815e-02, 1.4495e-04],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0., 0., 0.], device='cuda:0')\n",
      "Batch 8\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.6213e-03, 8.4127e-05, 9.8009e-01, 2.2726e-08, 1.8203e-02],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 0., 1.], device='cuda:0')\n",
      "Batch 9\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.8010e-01, 1.1768e-03, 5.9070e-04, 2.0316e-01, 6.1497e-01],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 1., 0.], device='cuda:0')\n",
      "Batch 10\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([6.3566e-03, 3.1874e-02, 1.8002e-05, 3.4453e-09, 9.6175e-01],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0., 0.], device='cuda:0')\n",
      "Batch 11\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([0.0079, 0.7091, 0.1979, 0.0028, 0.0823], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0., 0.], device='cuda:0')\n",
      "Batch 12\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.9369e-03, 4.6311e-06, 5.4993e-08, 9.9349e-01, 4.5649e-03],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0., 0., 0.], device='cuda:0')\n",
      "Batch 13\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.6143e-02, 5.4753e-05, 9.8375e-01, 5.0907e-05, 3.1465e-06],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 1., 0.], device='cuda:0')\n",
      "Batch 14\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.1224e-05, 2.2499e-02, 4.4259e-01, 5.1827e-01, 1.6626e-02],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0., 0.], device='cuda:0')\n",
      "Batch 15\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.8857e-03, 1.4259e-01, 8.5458e-01, 8.3784e-04, 9.9245e-05],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0., 0.], device='cuda:0')\n",
      "Batch 16\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([4.5616e-06, 2.0761e-02, 9.7922e-01, 1.0967e-05, 6.5740e-07],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0., 0.], device='cuda:0')\n",
      "Batch 17\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([0.0012, 0.0955, 0.1079, 0.7374, 0.0580], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 1., 0.], device='cuda:0')\n",
      "Batch 18\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.4337e-05, 2.0651e-03, 1.3734e-04, 2.0819e-10, 9.9778e-01],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0., 0.], device='cuda:0')\n",
      "Batch 19\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([2.5684e-07, 1.1656e-01, 7.1616e-03, 1.6592e-07, 8.7628e-01],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0., 0.], device='cuda:0')\n",
      "Batch 20\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([4.8993e-01, 4.8810e-01, 4.0912e-07, 9.2870e-03, 1.2690e-02],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0., 0., 0.], device='cuda:0')\n",
      "Batch 21\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.3679e-03, 1.2576e-01, 1.2319e-03, 1.8900e-05, 8.7162e-01],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0., 0.], device='cuda:0')\n",
      "Batch 22\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([9.7410e-06, 2.0315e-06, 2.8168e-07, 1.8374e-03, 9.9815e-01],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0., 0., 0.], device='cuda:0')\n",
      "Batch 23\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([5.5073e-02, 9.2164e-09, 3.9793e-01, 5.4699e-01, 6.0624e-06],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0., 0., 0.], device='cuda:0')\n",
      "Batch 24\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([9.9680e-01, 2.7707e-04, 1.4484e-07, 1.8196e-05, 2.9060e-03],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 0., 1.], device='cuda:0')\n",
      "Batch 25\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([8.2434e-07, 5.9597e-05, 9.5797e-01, 1.2180e-05, 4.1960e-02],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0., 0.], device='cuda:0')\n",
      "Batch 26\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([9.5624e-01, 6.4309e-06, 4.8205e-05, 2.1292e-04, 4.3493e-02],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0., 0., 0.], device='cuda:0')\n",
      "Batch 27\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([0.8309, 0.0103, 0.0403, 0.0009, 0.1176], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 0., 1.], device='cuda:0')\n",
      "Batch 28\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([3.0613e-07, 6.7785e-04, 9.9808e-01, 1.2371e-03, 6.9469e-08],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0., 0., 0.], device='cuda:0')\n",
      "Batch 29\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([2.4320e-02, 1.5450e-04, 4.1292e-01, 9.5696e-05, 5.6251e-01],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0., 0.], device='cuda:0')\n",
      "Batch 30\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([5.9964e-03, 3.6600e-04, 5.9887e-04, 9.9303e-01, 1.0465e-05],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0., 0.], device='cuda:0')\n",
      "Batch 31\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.4936e-03, 1.2138e-06, 9.0150e-01, 5.5977e-04, 9.6449e-02],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 0., 1.], device='cuda:0')\n",
      "Batch 32\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([2.4106e-04, 7.2028e-04, 4.3466e-02, 5.5744e-05, 9.5552e-01],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 1., 0.], device='cuda:0')\n",
      "Batch 33\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([2.5323e-01, 3.8530e-02, 1.1918e-03, 7.0702e-01, 2.5208e-05],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 1., 0.], device='cuda:0')\n",
      "Batch 34\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([6.3532e-04, 9.9398e-01, 3.6982e-05, 5.2766e-03, 7.0068e-05],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0., 0.], device='cuda:0')\n",
      "Batch 35\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([5.3272e-04, 1.0176e-03, 9.9342e-01, 7.4196e-09, 5.0319e-03],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 1., 0.], device='cuda:0')\n",
      "Batch 36\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([0.5815, 0.0103, 0.0323, 0.0007, 0.3751], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0., 0.], device='cuda:0')\n",
      "Batch 37\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.8965e-04, 9.9800e-01, 6.8039e-08, 1.8123e-03, 5.1647e-09],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 0., 1.], device='cuda:0')\n",
      "Batch 38\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([2.3350e-06, 1.2842e-02, 1.1331e-02, 9.7518e-01, 6.4072e-04],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0., 0.], device='cuda:0')\n",
      "Batch 39\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([0.1061, 0.0007, 0.5798, 0.2651, 0.0483], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 0., 1.], device='cuda:0')\n",
      "Batch 40\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([9.4770e-07, 1.1745e-06, 1.3037e-05, 9.9998e-01, 3.1265e-06],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0., 0., 0.], device='cuda:0')\n",
      "Batch 41\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([0.1478, 0.0035, 0.0027, 0.7620, 0.0839], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0., 0.], device='cuda:0')\n",
      "Batch 42\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.3995e-05, 1.8134e-03, 8.9400e-01, 1.0418e-01, 7.8779e-10],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 0., 1.], device='cuda:0')\n",
      "Batch 43\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([9.9999e-01, 6.7226e-10, 2.1604e-07, 1.3872e-05, 6.0806e-08],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0., 0., 0.], device='cuda:0')\n",
      "Batch 44\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([5.4678e-05, 9.9600e-01, 2.8787e-05, 3.8615e-03, 5.5997e-05],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0., 0.], device='cuda:0')\n",
      "Batch 45\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([3.1951e-01, 6.0692e-01, 7.3451e-02, 6.6401e-05, 5.1229e-05],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0., 0.], device='cuda:0')\n",
      "Batch 46\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.7007e-07, 2.4787e-06, 7.7030e-09, 1.0033e-07, 1.0000e+00],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0., 0.], device='cuda:0')\n",
      "Batch 47\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.8801e-04, 4.5243e-05, 2.1515e-06, 9.9968e-01, 8.4595e-05],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0., 0.], device='cuda:0')\n",
      "Batch 48\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([6.8228e-10, 2.9119e-02, 1.0048e-07, 9.7088e-01, 5.8749e-06],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0., 0., 0.], device='cuda:0')\n",
      "Batch 49\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([3.0876e-01, 1.2435e-03, 2.2132e-04, 2.6043e-02, 6.6373e-01],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0., 0.], device='cuda:0')\n",
      "Batch 50\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([9.9669e-01, 1.6049e-05, 2.4771e-03, 4.9286e-04, 3.2814e-04],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0., 0.], device='cuda:0')\n",
      "Batch 51\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([9.6705e-01, 4.7360e-05, 7.6702e-03, 3.1826e-05, 2.5204e-02],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0., 0., 0.], device='cuda:0')\n",
      "Batch 52\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([7.1118e-12, 9.6582e-11, 2.3159e-08, 1.0000e+00, 6.2768e-07],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 1., 0.], device='cuda:0')\n",
      "Batch 53\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([2.8958e-03, 1.4148e-04, 6.9256e-03, 2.3980e-03, 9.8764e-01],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0., 0.], device='cuda:0')\n",
      "Batch 54\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([9.9823e-05, 9.9911e-01, 1.4211e-04, 4.2816e-04, 2.2439e-04],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0., 0.], device='cuda:0')\n",
      "Batch 55\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.0136e-09, 2.9993e-12, 4.1880e-08, 2.3890e-10, 1.0000e+00],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 0., 1.], device='cuda:0')\n",
      "Batch 56\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([9.8746e-01, 3.7161e-05, 1.0209e-02, 2.1979e-03, 9.2894e-05],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0., 0.], device='cuda:0')\n",
      "Batch 57\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([2.3345e-06, 9.2391e-10, 9.9997e-01, 1.1872e-05, 1.1374e-05],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 0., 1.], device='cuda:0')\n",
      "Batch 58\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([9.1228e-07, 4.3488e-04, 1.4145e-03, 9.0009e-01, 9.8060e-02],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0., 0., 0.], device='cuda:0')\n",
      "Batch 59\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.8767e-09, 3.4850e-07, 1.0000e+00, 6.7047e-14, 6.2269e-08],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0., 0.], device='cuda:0')\n",
      "Batch 60\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([7.3101e-06, 2.2515e-02, 5.2947e-01, 5.8480e-04, 4.4743e-01],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 1., 0.], device='cuda:0')\n",
      "Batch 61\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([7.4221e-01, 2.4097e-01, 1.9172e-05, 2.3568e-05, 1.6784e-02],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 0., 1.], device='cuda:0')\n",
      "Batch 62\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([9.2570e-01, 3.5728e-03, 1.7501e-04, 1.5687e-05, 7.0534e-02],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0., 0.], device='cuda:0')\n",
      "Batch 63\n",
      "torch.Size([5, 10])\n",
      "torch.Size([30, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([64, 5])\n",
      "tensor([1.5624e-07, 1.1933e-02, 9.8789e-01, 4.4525e-05, 1.2931e-04],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0., 0.], device='cuda:0')\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "NPRATIO = 4\n",
    "\n",
    "# Move model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# Define the shapes of the input data\n",
    "his_input_title_shape = (HISTORY_SIZE, MAX_TITLE_LENGTH)\n",
    "pred_input_title_shape = (NPRATIO + 1, MAX_TITLE_LENGTH)\n",
    "label_shape = (NPRATIO + 1,)\n",
    "vocab_size = word2vec_embedding.shape[0]\n",
    "\n",
    "# Generate some random input data for input_1 with values between 0 and 1\n",
    "his_input_title = np.random.randint(0, vocab_size, (BATCH_SIZE, *his_input_title_shape))\n",
    "\n",
    "# Generate some random input data for input_2 with values between 0 and 1\n",
    "pred_input_title = np.random.randint(\n",
    "    0, vocab_size, (BATCH_SIZE, *pred_input_title_shape)\n",
    ")\n",
    "\n",
    "# Generate some random label data with values between 0 and 1\n",
    "label_data = np.zeros((BATCH_SIZE, *label_shape), dtype=int)\n",
    "for row in label_data:\n",
    "    row[np.random.choice(label_shape[0])] = 1\n",
    "\n",
    "print(HISTORY_SIZE)\n",
    "print(MAX_TITLE_LENGTH)\n",
    "print(NPRATIO)\n",
    "print(vocab_size)\n",
    "\n",
    "# Print the shapes of the input data to verify they match the model's input layers\n",
    "print(his_input_title.shape) \n",
    "print(pred_input_title.shape)\n",
    "print(label_data.shape)\n",
    "\n",
    "# Convert the input data to PyTorch tensors\n",
    "his_input_title = torch.from_numpy(his_input_title).long().to(device)\n",
    "pred_input_title = torch.from_numpy(pred_input_title).long().to(device)\n",
    "label_data = torch.from_numpy(label_data).float().to(device)\n",
    "\n",
    "# Zero the gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass\n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "    print(\"Batch\", i)\n",
    "    print(pred_input_title[i].shape)\n",
    "    print(his_input_title[i].shape)\n",
    "    outputs = model(pred_input_title[i], his_input_title[i])\n",
    "\n",
    "    print(outputs.shape)\n",
    "    print(label_data.shape)\n",
    "\n",
    "    print(outputs)\n",
    "    print(label_data[i])\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, label_data[i])\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "print(\"Done\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
