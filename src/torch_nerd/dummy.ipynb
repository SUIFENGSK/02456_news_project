{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1+cu124\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "BLACKHOLE = False\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" # fixes problem with graph\n",
    "import torch\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "\n",
    "# Check gpu availability\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Test:\n",
    "#print(torch.zeros(1).cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSModel(\n",
      "  (news_encoder): NewsEncoder(\n",
      "    (embedding): Embedding(1000, 100)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (self_attention): SelfAttention(\n",
      "      (query_proj): Linear(in_features=100, out_features=256, bias=True)\n",
      "      (key_proj): Linear(in_features=100, out_features=256, bias=True)\n",
      "      (value_proj): Linear(in_features=100, out_features=256, bias=True)\n",
      "    )\n",
      "    (dense_layers): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=200, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "      (4): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "      (7): Dropout(p=0.2, inplace=False)\n",
      "      (8): Linear(in_features=200, out_features=256, bias=True)\n",
      "      (9): ReLU()\n",
      "      (10): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (11): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (att_layer): AttLayer2(\n",
      "      (attention_projection): Linear(in_features=256, out_features=1000, bias=True)\n",
      "      (query_vector): Linear(in_features=1000, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (user_encoder): UserEncoder(\n",
      "    (title_encoder): NewsEncoder(\n",
      "      (embedding): Embedding(1000, 100)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (self_attention): SelfAttention(\n",
      "        (query_proj): Linear(in_features=100, out_features=256, bias=True)\n",
      "        (key_proj): Linear(in_features=100, out_features=256, bias=True)\n",
      "        (value_proj): Linear(in_features=100, out_features=256, bias=True)\n",
      "      )\n",
      "      (dense_layers): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=200, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "        (4): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (5): ReLU()\n",
      "        (6): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (7): Dropout(p=0.2, inplace=False)\n",
      "        (8): Linear(in_features=200, out_features=256, bias=True)\n",
      "        (9): ReLU()\n",
      "        (10): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (11): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (att_layer): AttLayer2(\n",
      "        (attention_projection): Linear(in_features=256, out_features=1000, bias=True)\n",
      "        (query_vector): Linear(in_features=1000, out_features=1, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (self_attention): SelfAttention(\n",
      "      (query_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (key_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (att_layer): AttLayer2(\n",
      "      (attention_projection): Linear(in_features=256, out_features=1000, bias=True)\n",
      "      (query_vector): Linear(in_features=1000, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (click_predictor): ClickPredictor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from nrms import NRMSModel\n",
    "from hyperparameters import hparams_nrms\n",
    "import numpy as np\n",
    "\n",
    "hparams = hparams_nrms()\n",
    "\n",
    "MAX_TITLE_LENGTH = 10\n",
    "HISTORY_SIZE = 50\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "word2vec_embedding = np.random.rand(1000, 100)\n",
    "\n",
    "# PARAMETERS\n",
    "hparams.title_size = MAX_TITLE_LENGTH\n",
    "hparams.history_size = HISTORY_SIZE\n",
    "hparams.batch_size = BATCH_SIZE\n",
    "hparams.candidate_size = 5\n",
    "\n",
    "# MODEL ARCHITECTURE\n",
    "hparams.head_num = 16\n",
    "hparams.head_dim = 16\n",
    "hparams.attention_hidden_dim = 1000\n",
    "hparams.linear_hidden_dim = 200\n",
    "hparams.embedding_dim = word2vec_embedding.shape[1]\n",
    "\n",
    "hparams.use_positional_encoding = False\n",
    "hparams.use_learned_positions = False\n",
    "\n",
    "# MODEL OPTIMIZER:\n",
    "hparams.optimizer = \"adam\"\n",
    "hparams.loss = \"cross_entropy_loss\"\n",
    "hparams.dropout = 0.2\n",
    "hparams.learning_rate = 1e-4\n",
    "\n",
    "model = NRMSModel(hparams=hparams, word2vec_embedding=word2vec_embedding, debug=True)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "if hparams.loss == \"cross_entropy_loss\":\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "elif hparams.loss == \"mse_loss\":\n",
    "    criterion = nn.MSELoss()\n",
    "else:\n",
    "    raise ValueError(f\"Loss function {hparams.loss} not supported\")\n",
    "\n",
    "if hparams.optimizer == \"adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hparams_nrms.learning_rate)\n",
    "else:\n",
    "    raise ValueError(f\"Optimizer {hparams.optimizer} not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "10\n",
      "4\n",
      "1000\n",
      "(64, 50, 10)\n",
      "(64, 5, 10)\n",
      "(64, 5)\n",
      "Model: Shape of pred_input_title: torch.Size([64, 5, 10]) . Should be (batch_size, candidate_size, title_size): (64, 5, 10)\n",
      "Model: Shape of his_input_title: torch.Size([64, 50, 10]) . Should be (batch_size, history_size, title_size): (64, 50, 10)\n",
      "UE1: Shape of input: torch.Size([64, 50, 10]) . Should be (batch_size, history_size, title_size): (64, 50, 10)\n",
      "NE0: Shape of input: torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE2: Shape after casting to long: torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE3: Shape after embedding: torch.Size([64, 10, 100]) . Should be (batch_size, title_size, embedding_dim): (64, 10, 100)\n",
      "NE4: Shape after dropout: torch.Size([64, 10, 100]) . Should be (batch_size, title_size, embedding_dim): (64, 10, 100)\n",
      "NE5: Shape after self attention: torch.Size([64, 10, 256]) . Should be (batch_size, title_size, head_num * head_dim): (64, 10, 256)\n",
      "NE6: Shape after dense layers: torch.Size([64, 10, 256]) . Should be (batch_size, title_size, head_num * head_dim): (64, 10, 256)\n",
      "NE7: Shape after att layer: torch.Size([64, 256]) . Should be (batch_size, head_num * head_dim): (64, 256)\n",
      "UE2: Shape after title encoder: torch.Size([64, 50, 256]) . Should be (batch_size, history_size, head_num * head_dim): (64, 50, 256)\n",
      "UE3: Shape after self attention  torch.Size([64, 50, 256]) . Should be (batch_size, history_size, head_num * head_dim): (64, 50, 256)\n",
      "UE4: Shape after att layer  torch.Size([64, 256]) . Should be (batch_size, head_num * head_dim): (64, 256)\n",
      "User representation done\n",
      "Model: Shape of pred_input_title after permutation: torch.Size([5, 64, 10]) . Should be (candidate_size, batch_size, title_size): (5, 64, 10)\n",
      "Model: Shape of pred_input_title 0 : torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE0: Shape of input: torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE2: Shape after casting to long: torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE3: Shape after embedding: torch.Size([64, 10, 100]) . Should be (batch_size, title_size, embedding_dim): (64, 10, 100)\n",
      "NE4: Shape after dropout: torch.Size([64, 10, 100]) . Should be (batch_size, title_size, embedding_dim): (64, 10, 100)\n",
      "NE5: Shape after self attention: torch.Size([64, 10, 256]) . Should be (batch_size, title_size, head_num * head_dim): (64, 10, 256)\n",
      "NE6: Shape after dense layers: torch.Size([64, 10, 256]) . Should be (batch_size, title_size, head_num * head_dim): (64, 10, 256)\n",
      "NE7: Shape after att layer: torch.Size([64, 256]) . Should be (batch_size, head_num * head_dim): (64, 256)\n",
      "Model: Shape of pred_input_title 1 : torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE0: Shape of input: torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE2: Shape after casting to long: torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE3: Shape after embedding: torch.Size([64, 10, 100]) . Should be (batch_size, title_size, embedding_dim): (64, 10, 100)\n",
      "NE4: Shape after dropout: torch.Size([64, 10, 100]) . Should be (batch_size, title_size, embedding_dim): (64, 10, 100)\n",
      "NE5: Shape after self attention: torch.Size([64, 10, 256]) . Should be (batch_size, title_size, head_num * head_dim): (64, 10, 256)\n",
      "NE6: Shape after dense layers: torch.Size([64, 10, 256]) . Should be (batch_size, title_size, head_num * head_dim): (64, 10, 256)\n",
      "NE7: Shape after att layer: torch.Size([64, 256]) . Should be (batch_size, head_num * head_dim): (64, 256)\n",
      "Model: Shape of pred_input_title 2 : torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE0: Shape of input: torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE2: Shape after casting to long: torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE3: Shape after embedding: torch.Size([64, 10, 100]) . Should be (batch_size, title_size, embedding_dim): (64, 10, 100)\n",
      "NE4: Shape after dropout: torch.Size([64, 10, 100]) . Should be (batch_size, title_size, embedding_dim): (64, 10, 100)\n",
      "NE5: Shape after self attention: torch.Size([64, 10, 256]) . Should be (batch_size, title_size, head_num * head_dim): (64, 10, 256)\n",
      "NE6: Shape after dense layers: torch.Size([64, 10, 256]) . Should be (batch_size, title_size, head_num * head_dim): (64, 10, 256)\n",
      "NE7: Shape after att layer: torch.Size([64, 256]) . Should be (batch_size, head_num * head_dim): (64, 256)\n",
      "Model: Shape of pred_input_title 3 : torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE0: Shape of input: torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE2: Shape after casting to long: torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE3: Shape after embedding: torch.Size([64, 10, 100]) . Should be (batch_size, title_size, embedding_dim): (64, 10, 100)\n",
      "NE4: Shape after dropout: torch.Size([64, 10, 100]) . Should be (batch_size, title_size, embedding_dim): (64, 10, 100)\n",
      "NE5: Shape after self attention: torch.Size([64, 10, 256]) . Should be (batch_size, title_size, head_num * head_dim): (64, 10, 256)\n",
      "NE6: Shape after dense layers: torch.Size([64, 10, 256]) . Should be (batch_size, title_size, head_num * head_dim): (64, 10, 256)\n",
      "NE7: Shape after att layer: torch.Size([64, 256]) . Should be (batch_size, head_num * head_dim): (64, 256)\n",
      "Model: Shape of pred_input_title 4 : torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE0: Shape of input: torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE2: Shape after casting to long: torch.Size([64, 10]) . Should be (batch_size, title_size): (64, 10)\n",
      "NE3: Shape after embedding: torch.Size([64, 10, 100]) . Should be (batch_size, title_size, embedding_dim): (64, 10, 100)\n",
      "NE4: Shape after dropout: torch.Size([64, 10, 100]) . Should be (batch_size, title_size, embedding_dim): (64, 10, 100)\n",
      "NE5: Shape after self attention: torch.Size([64, 10, 256]) . Should be (batch_size, title_size, head_num * head_dim): (64, 10, 256)\n",
      "NE6: Shape after dense layers: torch.Size([64, 10, 256]) . Should be (batch_size, title_size, head_num * head_dim): (64, 10, 256)\n",
      "NE7: Shape after att layer: torch.Size([64, 256]) . Should be (batch_size, head_num * head_dim): (64, 256)\n",
      "Model: Shape of user_representation: torch.Size([64, 256]) . Should be (batch_size, head_num * head_dim): (64, 256)\n",
      "Model: Shape of news_representations: torch.Size([5, 64, 256]) . Should be (candidate_size, batch_size, head_num * head_dim): (5, 64, 256)\n",
      "CP1: Shape of news_representation: torch.Size([5, 64, 256]) . Should be (batch_size, candidate_size, head_num * head_dim): (64, 5, 256)\n",
      "CP2: Shape of user_representation: torch.Size([64, 256]) . Should be (batch_size, head_num * head_dim): (64, 256)\n",
      "CP3: Reshape of news_representation: torch.Size([64, 5, 256]) . Should be (candidate_size, batch_size, head_num * head_dim): (5, 64, 256)\n",
      "CP3: Shape of prob: torch.Size([64, 5]) . Should be (batch_size, candidate_size): (64, 5)\n",
      "torch.Size([64, 5])\n",
      "torch.Size([64, 5])\n",
      "tensor([[0.1829, 0.2115, 0.1623, 0.2263, 0.2170],\n",
      "        [0.1909, 0.1932, 0.2294, 0.2491, 0.1374],\n",
      "        [0.1488, 0.3024, 0.1829, 0.1980, 0.1679],\n",
      "        [0.1964, 0.2432, 0.1755, 0.2290, 0.1559],\n",
      "        [0.2067, 0.1715, 0.2179, 0.2302, 0.1737],\n",
      "        [0.1413, 0.1840, 0.1815, 0.2663, 0.2269],\n",
      "        [0.1731, 0.1868, 0.1303, 0.2277, 0.2821],\n",
      "        [0.2015, 0.2161, 0.1755, 0.1956, 0.2114],\n",
      "        [0.1555, 0.3494, 0.2021, 0.1428, 0.1502],\n",
      "        [0.1467, 0.2061, 0.2177, 0.1864, 0.2431],\n",
      "        [0.1751, 0.2792, 0.2044, 0.2084, 0.1328],\n",
      "        [0.1918, 0.2719, 0.1774, 0.1678, 0.1911],\n",
      "        [0.1800, 0.2029, 0.2551, 0.1158, 0.2463],\n",
      "        [0.2695, 0.2049, 0.1472, 0.1927, 0.1856],\n",
      "        [0.2713, 0.1968, 0.1745, 0.1604, 0.1970],\n",
      "        [0.1937, 0.2237, 0.1613, 0.2541, 0.1672],\n",
      "        [0.1480, 0.2684, 0.1757, 0.1798, 0.2282],\n",
      "        [0.2324, 0.2320, 0.1649, 0.1966, 0.1741],\n",
      "        [0.1790, 0.1567, 0.2353, 0.1908, 0.2383],\n",
      "        [0.1712, 0.2477, 0.2367, 0.1016, 0.2428],\n",
      "        [0.2277, 0.1564, 0.1640, 0.1862, 0.2656],\n",
      "        [0.1740, 0.2459, 0.2574, 0.1527, 0.1700],\n",
      "        [0.1502, 0.1787, 0.2657, 0.2194, 0.1860],\n",
      "        [0.2415, 0.1875, 0.2031, 0.1612, 0.2067],\n",
      "        [0.2406, 0.2056, 0.1562, 0.1757, 0.2219],\n",
      "        [0.1618, 0.1692, 0.1745, 0.2873, 0.2072],\n",
      "        [0.2359, 0.1732, 0.1784, 0.1983, 0.2142],\n",
      "        [0.1464, 0.1883, 0.2157, 0.2071, 0.2425],\n",
      "        [0.2374, 0.1699, 0.2440, 0.2001, 0.1486],\n",
      "        [0.1796, 0.1699, 0.1998, 0.2231, 0.2277],\n",
      "        [0.2151, 0.1623, 0.2622, 0.1217, 0.2388],\n",
      "        [0.2551, 0.1690, 0.1856, 0.1936, 0.1967],\n",
      "        [0.1950, 0.2245, 0.1794, 0.1776, 0.2234],\n",
      "        [0.1727, 0.1939, 0.2380, 0.1916, 0.2039],\n",
      "        [0.2422, 0.1906, 0.1321, 0.1688, 0.2662],\n",
      "        [0.1685, 0.2325, 0.2327, 0.2097, 0.1566],\n",
      "        [0.2249, 0.2280, 0.1720, 0.1936, 0.1815],\n",
      "        [0.1704, 0.2356, 0.2819, 0.1868, 0.1254],\n",
      "        [0.1867, 0.2457, 0.1872, 0.1671, 0.2132],\n",
      "        [0.2071, 0.1541, 0.2425, 0.1733, 0.2230],\n",
      "        [0.1631, 0.2112, 0.1441, 0.2754, 0.2062],\n",
      "        [0.2684, 0.1369, 0.1526, 0.1578, 0.2843],\n",
      "        [0.1834, 0.1840, 0.2306, 0.1931, 0.2089],\n",
      "        [0.2068, 0.2455, 0.1743, 0.2034, 0.1701],\n",
      "        [0.1995, 0.2126, 0.1255, 0.2761, 0.1864],\n",
      "        [0.2193, 0.1573, 0.3045, 0.1436, 0.1752],\n",
      "        [0.1497, 0.2140, 0.1625, 0.3113, 0.1624],\n",
      "        [0.2373, 0.1347, 0.1996, 0.1724, 0.2560],\n",
      "        [0.2104, 0.1757, 0.1476, 0.2914, 0.1749],\n",
      "        [0.1459, 0.2456, 0.2426, 0.1953, 0.1706],\n",
      "        [0.1363, 0.2263, 0.2243, 0.2199, 0.1933],\n",
      "        [0.1970, 0.1993, 0.2312, 0.1864, 0.1861],\n",
      "        [0.1657, 0.2079, 0.1262, 0.2682, 0.2320],\n",
      "        [0.2332, 0.1721, 0.1734, 0.2581, 0.1632],\n",
      "        [0.1176, 0.1838, 0.2644, 0.2639, 0.1703],\n",
      "        [0.1353, 0.2276, 0.1955, 0.1692, 0.2723],\n",
      "        [0.1794, 0.3109, 0.1126, 0.1980, 0.1992],\n",
      "        [0.2136, 0.1829, 0.2255, 0.1704, 0.2076],\n",
      "        [0.2173, 0.1590, 0.2136, 0.1596, 0.2505],\n",
      "        [0.1817, 0.2047, 0.2363, 0.1651, 0.2122],\n",
      "        [0.2378, 0.2036, 0.1630, 0.1834, 0.2121],\n",
      "        [0.1513, 0.2677, 0.1769, 0.2129, 0.1913],\n",
      "        [0.1606, 0.2058, 0.1887, 0.2634, 0.1816],\n",
      "        [0.2142, 0.1802, 0.2187, 0.1650, 0.2220]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]], device='cuda:0')\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "NPRATIO = 4\n",
    "\n",
    "# Move model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# Define the shapes of the input data\n",
    "his_input_title_shape = (HISTORY_SIZE, MAX_TITLE_LENGTH)\n",
    "pred_input_title_shape = (NPRATIO + 1, MAX_TITLE_LENGTH)\n",
    "label_shape = (NPRATIO + 1,)\n",
    "vocab_size = word2vec_embedding.shape[0]\n",
    "\n",
    "# Generate some random input data for input_1 with values between 0 and 1\n",
    "his_input_title = np.random.randint(0, vocab_size, (BATCH_SIZE, *his_input_title_shape))\n",
    "\n",
    "# Generate some random input data for input_2 with values between 0 and 1\n",
    "pred_input_title = np.random.randint(\n",
    "    0, vocab_size, (BATCH_SIZE, *pred_input_title_shape)\n",
    ")\n",
    "\n",
    "# Generate some random label data with values between 0 and 1\n",
    "label_data = np.zeros((BATCH_SIZE, *label_shape), dtype=int)\n",
    "for row in label_data:\n",
    "    row[np.random.choice(label_shape[0])] = 1\n",
    "\n",
    "print(HISTORY_SIZE)\n",
    "print(MAX_TITLE_LENGTH)\n",
    "print(NPRATIO)\n",
    "print(vocab_size)\n",
    "\n",
    "# Print the shapes of the input data to verify they match the model's input layers\n",
    "print(his_input_title.shape) \n",
    "print(pred_input_title.shape)\n",
    "print(label_data.shape)\n",
    "\n",
    "# Convert the input data to PyTorch tensors\n",
    "his_input_title = torch.from_numpy(his_input_title).long().to(device)\n",
    "pred_input_title = torch.from_numpy(pred_input_title).long().to(device)\n",
    "label_data = torch.from_numpy(label_data).float().to(device)\n",
    "\n",
    "# Zero the gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(pred_input_title, his_input_title)\n",
    "\n",
    "print(outputs.shape)\n",
    "print(label_data.shape)\n",
    "\n",
    "print(outputs)\n",
    "print(label_data)\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(outputs, label_data)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Update the weights\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "print(\"Done\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
