{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:27:24.787221Z",
     "start_time": "2024-12-03T14:27:24.779871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BLACKHOLE = True\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" # fixes problem with graph\n",
    "\n",
    "\n",
    "if BLACKHOLE:\n",
    "    workspace_path = os.path.expandvars('$BLACKHOLE')\n",
    "    sys.path.append(workspace_path+'/DeepLearning/02456_news_project/src')\n",
    "    DATAPATH = Path(workspace_path+\"/DeepLearning/ebnerd_data\").expanduser()\n",
    "else:\n",
    "    DATAPATH = Path(\"~/ebnerd_data\").expanduser()\n",
    "\n",
    "DATASET = \"ebnerd_demo\"\n",
    "#DATASET = \"ebnerd_small\"\n",
    "#DATASET = \"ebnerd_large\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:27:26.648127Z",
     "start_time": "2024-12-03T14:27:24.823993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "\n",
    "# Check gpu availability\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Test:\n",
    "#print(torch.zeros(1).cuda())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.1.2+cu121\n",
      "cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:27:27.609956Z",
     "start_time": "2024-12-03T14:27:26.826375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.data_handler import NewsDataset\n",
    "import from_ebrec._constants as cs\n",
    "\n",
    "SEED = 65\n",
    "HISTORY_SIZE = 50\n",
    "CANDITATE_SIZE = 5\n",
    "\n",
    "COLS = [\n",
    "    cs.DEFAULT_USER_COL,\n",
    "    cs.DEFAULT_IMPRESSION_ID_COL,\n",
    "    cs.DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    cs.DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    cs.DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    cs.DEFAULT_INVIEW_ARTICLES_COL,\n",
    "]\n",
    "\n",
    "#FRACTION = 0.001\n",
    "FRACTION = 0.01\n",
    "#FRACTION = 0.1\n",
    "#FRACTION = 1\n",
    "\n",
    "# test\n",
    "dataset = NewsDataset()\n",
    "\n",
    "dataset.setup_df(dataset_path = DATAPATH, datasplit = DATASET, history_size = HISTORY_SIZE, columns = COLS, fraction = FRACTION, seed = SEED, candidate_size=CANDITATE_SIZE)\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:28:52.320800Z",
     "start_time": "2024-12-03T14:27:27.617601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import transformers as huggingface\n",
    "from from_ebrec._nlp import get_transformers_word_embeddings\n",
    "from from_ebrec._polars import concat_str_columns\n",
    "from from_ebrec._articles import convert_text2encoding_with_transformers\n",
    "from from_ebrec._articles import create_article_id_to_value_mapping\n",
    "\n",
    "dataset.setup_articles_data(dataset_path = DATAPATH.joinpath(DATASET))\n",
    "\n",
    "df_articles = dataset.df_articles\n",
    "\n",
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "TEXT_COLUMNS_TO_USE = [cs.DEFAULT_SUBTITLE_COL, cs.DEFAULT_TITLE_COL, cs.DEFAULT_ARTICLE_PUBLISHED_TIMESTAMP_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = huggingface.AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = huggingface.AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH)\n",
    "article_mapping = create_article_id_to_value_mapping(df=df_articles, value_col=token_col_title)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dtu/blackhole/0f/168015/DeepLearning/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/dtu/blackhole/0f/168015/DeepLearning/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:28:52.470721Z",
     "start_time": "2024-12-03T14:28:52.449738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nrms import NRMSModel\n",
    "from hyperparameters import hparams_nrms\n",
    "\n",
    "hparams = hparams_nrms()\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "# PARAMETERS\n",
    "hparams.title_size = MAX_TITLE_LENGTH\n",
    "hparams.history_size = HISTORY_SIZE\n",
    "hparams.batch_size = BATCH_SIZE\n",
    "hparams.candidate_size = CANDITATE_SIZE\n",
    "\n",
    "# MODEL ARCHITECTURE\n",
    "hparams.head_num = 16\n",
    "hparams.head_dim = 16\n",
    "hparams.attention_hidden_dim = 200\n",
    "hparams.linear_hidden_dim = 200\n",
    "hparams.embedding_dim = word2vec_embedding.shape[1]\n",
    "\n",
    "hparams.use_positional_encoding = False\n",
    "hparams.use_learned_positions = False\n",
    "\n",
    "# MODEL OPTIMIZER:\n",
    "hparams.optimizer = \"adam\"\n",
    "hparams.loss = \"mse_loss\"\n",
    "hparams.dropout = 0.2\n",
    "hparams.learning_rate = 1e-4\n",
    "\n",
    "model = NRMSModel(hparams=hparams, word2vec_embedding=word2vec_embedding, debug=False)\n",
    "\n",
    "print(model)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSModel(\n",
      "  (news_encoder): NewsEncoder(\n",
      "    (embedding): Embedding(250002, 768)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (self_attention): SelfAttention(\n",
      "      (query_proj): Linear(in_features=768, out_features=256, bias=True)\n",
      "      (key_proj): Linear(in_features=768, out_features=256, bias=True)\n",
      "      (value_proj): Linear(in_features=768, out_features=256, bias=True)\n",
      "    )\n",
      "    (dense_layers): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=200, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "      (4): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "      (7): Dropout(p=0.2, inplace=False)\n",
      "      (8): Linear(in_features=200, out_features=256, bias=True)\n",
      "      (9): ReLU()\n",
      "      (10): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (11): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (att_layer): AttLayer2(\n",
      "      (attention_projection): Linear(in_features=256, out_features=200, bias=True)\n",
      "      (query_vector): Linear(in_features=200, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (user_encoder): UserEncoder(\n",
      "    (title_encoder): NewsEncoder(\n",
      "      (embedding): Embedding(250002, 768)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (self_attention): SelfAttention(\n",
      "        (query_proj): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (key_proj): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (value_proj): Linear(in_features=768, out_features=256, bias=True)\n",
      "      )\n",
      "      (dense_layers): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=200, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "        (4): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (5): ReLU()\n",
      "        (6): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (7): Dropout(p=0.2, inplace=False)\n",
      "        (8): Linear(in_features=200, out_features=256, bias=True)\n",
      "        (9): ReLU()\n",
      "        (10): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (11): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (att_layer): AttLayer2(\n",
      "        (attention_projection): Linear(in_features=256, out_features=200, bias=True)\n",
      "        (query_vector): Linear(in_features=200, out_features=1, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (self_attention): SelfAttention(\n",
      "      (query_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (key_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (att_layer): AttLayer2(\n",
      "      (attention_projection): Linear(in_features=256, out_features=200, bias=True)\n",
      "      (query_vector): Linear(in_features=200, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (click_predictor): ClickPredictor()\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:28:52.917678Z",
     "start_time": "2024-12-03T14:28:52.476285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "if hparams.loss == \"cross_entropy_loss\":\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "elif hparams.loss == \"mse_loss\":\n",
    "    criterion = nn.MSELoss()\n",
    "else:\n",
    "    raise ValueError(f\"Loss function {hparams.loss} not supported\")\n",
    "\n",
    "if hparams.optimizer == \"adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hparams_nrms.learning_rate)\n",
    "else:\n",
    "    raise ValueError(f\"Optimizer {hparams.optimizer} not supported\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:28:53.960881Z",
     "start_time": "2024-12-03T14:28:52.975063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataloader import NRMSDataLoader\n",
    "\n",
    "\n",
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors= dataset.df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column= cs.DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors= dataset.df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column= cs.DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:29:09.377116Z",
     "start_time": "2024-12-03T14:28:54.192009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model \n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# Move model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "train_loss_history, val_loss_history = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    first_batch = True\n",
    "    for iteration, (data, labels) in enumerate(train_dataloader):\n",
    "        # Unpacking of batch\n",
    "        his_input_title, pred_input_title, time_stamp = data\n",
    "\n",
    "        # Move data to device\n",
    "        his_input_title = his_input_title.to(device)\n",
    "        pred_input_title = pred_input_title.to(device)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pred_input_title, his_input_title)  \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}, Train iteration {iteration + 1}/{len(train_dataloader)}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for iteration, (data, labels) in enumerate(val_dataloader):\n",
    "            his_input_title, pred_input_title, time_stamp = data\n",
    "\n",
    "            his_input_title = his_input_title.to(device)\n",
    "            pred_input_title = pred_input_title.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(pred_input_title, his_input_title)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{EPOCHS}, Val iteration {iteration + 1}/{len(val_dataloader)}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    val_loss /= len(val_dataloader)\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6847e+09, 1.6846e+09, 1.6847e+09, 1.6848e+09, 1.6849e+09, 1.6849e+09,\n",
      "        1.6848e+09, 1.6847e+09, 1.6849e+09, 1.6847e+09, 1.6849e+09, 1.6849e+09,\n",
      "        1.6848e+09, 1.6847e+09, 1.6848e+09, 1.6845e+09, 1.6845e+09, 1.6848e+09,\n",
      "        1.6847e+09, 1.6848e+09, 1.6845e+09, 1.6844e+09, 1.6845e+09, 1.6848e+09,\n",
      "        1.6848e+09, 1.6846e+09, 1.6846e+09, 1.6847e+09, 1.6848e+09, 1.6844e+09,\n",
      "        1.6845e+09, 1.6847e+09, 1.6847e+09, 1.6848e+09, 1.6846e+09, 1.6846e+09,\n",
      "        1.6845e+09, 1.6844e+09, 1.6848e+09, 1.6844e+09, 1.6847e+09, 1.6846e+09,\n",
      "        1.6846e+09, 1.6844e+09, 1.6846e+09, 1.6849e+09, 1.6844e+09, 1.6849e+09,\n",
      "        1.6845e+09, 1.6845e+09, 1.6846e+09, 1.6845e+09, 1.6847e+09, 1.6847e+09,\n",
      "        1.6847e+09, 1.6845e+09, 1.6849e+09, 1.6847e+09, 1.6847e+09, 1.6846e+09,\n",
      "        1.6846e+09, 1.6848e+09, 1.6847e+09, 1.6846e+09])\n",
      "Epoch 1/10, Train iteration 1/4: Loss = 0.1601\n",
      "tensor([1.6845e+09, 1.6846e+09, 1.6847e+09, 1.6844e+09, 1.6844e+09, 1.6847e+09,\n",
      "        1.6844e+09, 1.6848e+09, 1.6846e+09, 1.6845e+09, 1.6844e+09, 1.6848e+09,\n",
      "        1.6848e+09, 1.6845e+09, 1.6848e+09, 1.6848e+09, 1.6846e+09, 1.6846e+09,\n",
      "        1.6846e+09, 1.6846e+09, 1.6848e+09, 1.6848e+09, 1.6844e+09, 1.6849e+09,\n",
      "        1.6846e+09, 1.6847e+09, 1.6848e+09, 1.6845e+09, 1.6846e+09, 1.6846e+09,\n",
      "        1.6846e+09, 1.6844e+09, 1.6847e+09, 1.6845e+09, 1.6845e+09, 1.6846e+09,\n",
      "        1.6848e+09, 1.6848e+09, 1.6847e+09, 1.6848e+09, 1.6846e+09, 1.6846e+09,\n",
      "        1.6847e+09, 1.6847e+09, 1.6846e+09, 1.6844e+09, 1.6844e+09, 1.6848e+09,\n",
      "        1.6849e+09, 1.6845e+09, 1.6847e+09, 1.6845e+09, 1.6847e+09, 1.6847e+09,\n",
      "        1.6848e+09, 1.6849e+09, 1.6848e+09, 1.6848e+09, 1.6845e+09, 1.6847e+09,\n",
      "        1.6847e+09, 1.6847e+09, 1.6846e+09, 1.6847e+09])\n",
      "Epoch 1/10, Train iteration 2/4: Loss = 0.1602\n",
      "tensor([1.6846e+09, 1.6847e+09, 1.6846e+09, 1.6846e+09, 1.6846e+09, 1.6844e+09,\n",
      "        1.6849e+09, 1.6849e+09, 1.6849e+09, 1.6847e+09, 1.6845e+09, 1.6847e+09,\n",
      "        1.6848e+09, 1.6845e+09, 1.6847e+09, 1.6844e+09, 1.6848e+09, 1.6846e+09,\n",
      "        1.6846e+09, 1.6844e+09, 1.6844e+09, 1.6848e+09, 1.6846e+09, 1.6846e+09,\n",
      "        1.6846e+09, 1.6848e+09, 1.6848e+09, 1.6845e+09, 1.6844e+09, 1.6845e+09,\n",
      "        1.6847e+09, 1.6845e+09, 1.6846e+09, 1.6847e+09, 1.6848e+09, 1.6848e+09,\n",
      "        1.6847e+09, 1.6844e+09, 1.6846e+09, 1.6846e+09, 1.6846e+09, 1.6845e+09,\n",
      "        1.6849e+09, 1.6849e+09, 1.6848e+09, 1.6848e+09, 1.6846e+09, 1.6847e+09,\n",
      "        1.6844e+09, 1.6847e+09, 1.6848e+09, 1.6848e+09, 1.6848e+09, 1.6848e+09,\n",
      "        1.6847e+09, 1.6845e+09, 1.6845e+09, 1.6845e+09, 1.6849e+09, 1.6844e+09,\n",
      "        1.6848e+09, 1.6846e+09, 1.6845e+09, 1.6848e+09])\n",
      "Epoch 1/10, Train iteration 3/4: Loss = 0.1598\n",
      "tensor([1.6847e+09, 1.6846e+09, 1.6845e+09, 1.6847e+09, 1.6848e+09, 1.6844e+09,\n",
      "        1.6844e+09, 1.6847e+09, 1.6845e+09, 1.6846e+09, 1.6844e+09, 1.6846e+09,\n",
      "        1.6847e+09, 1.6847e+09, 1.6845e+09, 1.6847e+09, 1.6846e+09, 1.6847e+09,\n",
      "        1.6848e+09, 1.6847e+09, 1.6848e+09, 1.6848e+09])\n",
      "Epoch 1/10, Train iteration 4/4: Loss = 0.1601\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 44\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m iteration, (data, labels) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(val_dataloader):\n\u001B[0;32m---> 44\u001B[0m         his_input_title, pred_input_title \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m     46\u001B[0m         his_input_title \u001B[38;5;241m=\u001B[39m his_input_title\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     47\u001B[0m         pred_input_title \u001B[38;5;241m=\u001B[39m pred_input_title\u001B[38;5;241m.\u001B[39mto(device)\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot the loss history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "plt.plot(val_loss_history, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate the model\n",
    "BATCH_SIZE_TEST = 10\n",
    "\n",
    "dataset.setup_test_data(dataset_path = DATAPATH, datasplit = DATASET, history_size = HISTORY_SIZE, columns = COLS, fraction = FRACTION, seed = SEED, candidate_size=CANDITATE_SIZE)\n",
    "\n",
    "test_dataloader = NRMSDataLoader(\n",
    "    behaviors=dataset.df_test,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=cs.DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "pred_test = []\n",
    "labels_test = []\n",
    "with torch.no_grad():  \n",
    "    for iteration, (data, labels) in enumerate(test_dataloader):\n",
    "        his_input_title, pred_input_title, time_stamp = data\n",
    "\n",
    "        his_input_title = his_input_title.to(device)\n",
    "        pred_input_title = pred_input_title.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(pred_input_title, his_input_title) \n",
    "        loss = criterion(outputs, labels)                 \n",
    "        test_loss += loss.item()\n",
    "\n",
    "        for i in range(outputs.size(0)):\n",
    "            pred_test.append(outputs[i].tolist())\n",
    "            labels_test.append(labels[i].tolist())\n",
    "\n",
    "        print(f\"Test Batch {iteration + 1}/{len(test_dataloader)}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "    print(\"Test loss:\", test_loss)\n",
    "        \n",
    "print(pred_test)\n",
    "print(labels_test)\n",
    "\n",
    "from from_ebrec.evaluation import MetricEvaluator\n",
    "from from_ebrec.evaluation import AucScore, MrrScore, NdcgScore\n",
    "metrics = MetricEvaluator(\n",
    "    labels = labels_test,\n",
    "    predictions= pred_test,\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()\n",
    "print(metrics)\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "number_to_print = 20\n",
    "print(\"Top %d predictions vs labels:\" % number_to_print)\n",
    "labels = dataset.df_test[\"labels\"].to_list()\n",
    "for i in range(number_to_print):\n",
    "    print(f\"Article {i}\")\n",
    "    for j in range(len(pred_test[i])):\n",
    "        print(f\"{pred_test[i][j]:.3f} vs {labels[i][j]:.3f}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Flatten the data for analysis\n",
    "predicted_probabilities = [prob for article in pred_test for prob in article]\n",
    "true_values = [val for article in labels[:len(pred_test)] for val in article]\n",
    "\n",
    "\n",
    "# Set a threshold (commonly 0.5) to classify probabilities as 0 or 1\n",
    "threshold = 0.5\n",
    "predicted_classes = [1 if p >= threshold else 0 for p in predicted_probabilities]\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_values, predicted_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate AUC and ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(true_values, predicted_classes)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve with AUC value explicitly highlighted\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label=\"Random Guess\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
