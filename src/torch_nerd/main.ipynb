{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:43:12.790212Z",
     "start_time": "2024-12-08T13:43:12.783359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BLACKHOLE = True\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" # fixes problem with graph\n",
    "\n",
    "\n",
    "if BLACKHOLE:\n",
    "    workspace_path = os.path.expandvars('$BLACKHOLE')\n",
    "    sys.path.append(workspace_path+'/DeepLearning/02456_news_project/src')\n",
    "    DATAPATH = Path(workspace_path+\"/DeepLearning/ebnerd_data\").expanduser()\n",
    "else:\n",
    "    DATAPATH = Path(\"~/ebnerd_data\").expanduser()\n",
    "\n",
    "#DATASET = \"ebnerd_demo\"\n",
    "#DATASET = \"ebnerd_small\"\n",
    "DATASET = \"ebnerd_large\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:43:14.502255Z",
     "start_time": "2024-12-08T13:43:12.826475Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "\n",
    "# Check gpu availability\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Test:\n",
    "#print(torch.zeros(1).cuda())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.1.2+cu121\n",
      "cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:43:15.398626Z",
     "start_time": "2024-12-08T13:43:14.688197Z"
    }
   },
   "source": [
    "from utils.data_handler import NewsDataset\n",
    "import from_ebrec._constants as cs\n",
    "\n",
    "SEED = 42\n",
    "HISTORY_SIZE = 100\n",
    "CANDITATE_SIZE = 5\n",
    "\n",
    "COLS = [\n",
    "    cs.DEFAULT_USER_COL,\n",
    "    cs.DEFAULT_IMPRESSION_ID_COL,\n",
    "    cs.DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    cs.DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    cs.DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    cs.DEFAULT_INVIEW_ARTICLES_COL,\n",
    "]\n",
    "\n",
    "FRACTION = 0.01\n",
    "#FRACTION = 0.01\n",
    "#FRACTION = 0.1\n",
    "#FRACTION = 1\n",
    "\n",
    "# test\n",
    "dataset = NewsDataset()\n",
    "\n",
    "dataset.setup_df(dataset_path = DATAPATH, datasplit = DATASET, history_size = HISTORY_SIZE, columns = COLS, fraction = FRACTION, seed = SEED, candidate_size=CANDITATE_SIZE)\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:44:39.537808Z",
     "start_time": "2024-12-08T13:43:15.405866Z"
    }
   },
   "source": [
    "import transformers as huggingface\n",
    "from from_ebrec._nlp import get_transformers_word_embeddings\n",
    "from from_ebrec._polars import concat_str_columns\n",
    "from from_ebrec._articles import convert_text2encoding_with_transformers\n",
    "from from_ebrec._articles import create_article_id_to_value_mapping\n",
    "\n",
    "dataset.setup_articles_data(dataset_path = DATAPATH.joinpath(DATASET))\n",
    "\n",
    "df_articles = dataset.df_articles\n",
    "\n",
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-large\"\n",
    "TEXT_COLUMNS_TO_USE = [cs.DEFAULT_SUBTITLE_COL, cs.DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = huggingface.AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = huggingface.AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH)\n",
    "article_mapping = create_article_id_to_value_mapping(df=df_articles, value_col=token_col_title)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dtu/blackhole/0f/168015/DeepLearning/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/dtu/blackhole/0f/168015/DeepLearning/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:44:39.744604Z",
     "start_time": "2024-12-08T13:44:39.646644Z"
    }
   },
   "source": [
    "from nrms import NRMSModel\n",
    "from hyperparameters import hparams_nrms\n",
    "\n",
    "hparams = hparams_nrms()\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# PARAMETERS\n",
    "hparams.title_size = MAX_TITLE_LENGTH\n",
    "hparams.history_size = HISTORY_SIZE\n",
    "hparams.batch_size = BATCH_SIZE\n",
    "hparams.candidate_size = CANDITATE_SIZE\n",
    "\n",
    "# MODEL ARCHITECTURE\n",
    "hparams.head_num = 32\n",
    "hparams.head_dim = 32\n",
    "hparams.attention_hidden_dim = 200\n",
    "hparams.linear_hidden_dim = 2000\n",
    "hparams.embedding_dim = word2vec_embedding.shape[1]\n",
    "\n",
    "hparams.use_positional_encoding = True\n",
    "\n",
    "hparams.use_time_embedding = False\n",
    "hparams.time_dim = 1\n",
    "hparams.time_embedding_dim = 32\n",
    "\n",
    "# MODEL OPTIMIZER:\n",
    "hparams.optimizer = \"adamw\"\n",
    "hparams.loss = \"mse_loss\"\n",
    "hparams.dropout = 0.3\n",
    "hparams.learning_rate = 1e-3\n",
    "hparams.weight_decay = 1e-4\n",
    "hparams.momentum = 0.9\n",
    "\n",
    "model = NRMSModel(hparams=hparams, word2vec_embedding=word2vec_embedding)\n",
    "\n",
    "print(model)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSModel(\n",
      "  (news_encoder): NewsEncoder(\n",
      "    (embedding): Embedding(250002, 1024)\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (positional_encoder): PositionEncoder(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (self_attention): SelfAttention(\n",
      "      (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dense_layers): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=2000, bias=True)\n",
      "      (1): LayerNorm((2000,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "      (4): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "      (5): LayerNorm((2000,), eps=1e-05, elementwise_affine=True)\n",
      "      (6): GELU(approximate='none')\n",
      "      (7): Dropout(p=0.3, inplace=False)\n",
      "      (8): Linear(in_features=2000, out_features=1024, bias=True)\n",
      "      (9): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (10): GELU(approximate='none')\n",
      "      (11): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (att_layer): AttLayer2(\n",
      "      (attention_projection): Linear(in_features=1024, out_features=200, bias=True)\n",
      "      (query_vector): Linear(in_features=200, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (user_encoder): UserEncoder(\n",
      "    (news_encoder): NewsEncoder(\n",
      "      (embedding): Embedding(250002, 1024)\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (positional_encoder): PositionEncoder(\n",
      "        (dropout): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "      (self_attention): SelfAttention(\n",
      "        (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (dense_layers): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=2000, bias=True)\n",
      "        (1): LayerNorm((2000,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Dropout(p=0.3, inplace=False)\n",
      "        (4): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "        (5): LayerNorm((2000,), eps=1e-05, elementwise_affine=True)\n",
      "        (6): GELU(approximate='none')\n",
      "        (7): Dropout(p=0.3, inplace=False)\n",
      "        (8): Linear(in_features=2000, out_features=1024, bias=True)\n",
      "        (9): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (10): GELU(approximate='none')\n",
      "        (11): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "      (att_layer): AttLayer2(\n",
      "        (attention_projection): Linear(in_features=1024, out_features=200, bias=True)\n",
      "        (query_vector): Linear(in_features=200, out_features=1, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (self_attention): SelfAttention(\n",
      "      (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (att_layer): AttLayer2(\n",
      "      (attention_projection): Linear(in_features=1024, out_features=200, bias=True)\n",
      "      (query_vector): Linear(in_features=200, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (click_predictor): ClickPredictor()\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:44:39.752554Z",
     "start_time": "2024-12-08T13:44:39.748266Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "print(\"GPU =\",torch.cuda.device_count())\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = nn.DataParallel(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = 0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:44:40.160913Z",
     "start_time": "2024-12-08T13:44:39.862598Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "if hparams.loss == \"cross_entropy_loss\":\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "elif hparams.loss == \"mse_loss\":\n",
    "    criterion = nn.MSELoss()\n",
    "else:\n",
    "    raise ValueError(f\"Loss function {hparams.loss} not supported\")\n",
    "\n",
    "if hparams.optimizer == \"adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hparams_nrms.learning_rate, weight_decay=hparams_nrms.weight_decay)\n",
    "elif hparams.optimizer == \"adamw\":\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=hparams_nrms.learning_rate, weight_decay=hparams_nrms.weight_decay)\n",
    "elif hparams.optimizer == \"sgd\":\n",
    "    optimizer = optim.SGD(model.parameters(), lr=hparams_nrms.learning_rate, momentum=hparams_nrms.momentum)\n",
    "else:\n",
    "    raise ValueError(f\"Optimizer {hparams.optimizer} not supported\")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:44:41.067568Z",
     "start_time": "2024-12-08T13:44:40.165266Z"
    }
   },
   "source": [
    "from dataloader import NRMSDataLoader\n",
    "\n",
    "\n",
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors= dataset.df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column= cs.DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors= dataset.df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column= cs.DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:44:41.287608Z",
     "start_time": "2024-12-08T13:44:41.284790Z"
    }
   },
   "source": [
    "# dynamic learning rate\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',  # Minimizing validation loss\n",
    "    factor=0.3,  # Reduce the learning rate by half\n",
    "    patience=2,  # Wait 2 epochs without improvement\n",
    "    verbose=True  # Log the learning rate changes\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:45:11.637226Z",
     "start_time": "2024-12-08T13:44:41.503914Z"
    }
   },
   "source": [
    "# Train the model\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "# Move model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "train_loss_history, val_loss_history = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    for iteration, (data, labels) in enumerate(train_dataloader):\n",
    "        # Unpacking of batch\n",
    "        his_input_title, pred_input_title, timestamps = data\n",
    "\n",
    "        # Move data to device\n",
    "        his_input_title = his_input_title.to(device)\n",
    "        pred_input_title = pred_input_title.to(device)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pred_input_title, his_input_title)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}, Train iteration {iteration + 1}/{len(train_dataloader)}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for iteration, (data, labels) in enumerate(val_dataloader):\n",
    "            his_input_title, pred_input_title, timestamps = data\n",
    "\n",
    "            his_input_title = his_input_title.to(device)\n",
    "            pred_input_title = pred_input_title.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(pred_input_title, his_input_title)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{EPOCHS}, Val iteration {iteration + 1}/{len(val_dataloader)}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    val_loss /= len(val_dataloader)\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    # Log current learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"Current learning rate: {param_group['lr']}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train iteration 1/4: Loss = 0.1775\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 30\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n\u001B[1;32m     29\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 30\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     32\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m/dtu/blackhole/0f/168015/DeepLearning/.venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/dtu/blackhole/0f/168015/DeepLearning/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the loss history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "plt.plot(val_loss_history, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate the model\n",
    "BATCH_SIZE_TEST = 10\n",
    "\n",
    "dataset.setup_test_data(dataset_path = DATAPATH, datasplit = DATASET, history_size = HISTORY_SIZE, columns = COLS, fraction = FRACTION, seed = SEED, candidate_size=CANDITATE_SIZE)\n",
    "\n",
    "test_dataloader = NRMSDataLoader(\n",
    "    behaviors=dataset.df_test,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=cs.DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "pred_test = []\n",
    "labels_test = []\n",
    "with torch.no_grad():\n",
    "    for iteration, (data, labels) in enumerate(test_dataloader):\n",
    "        his_input_title, pred_input_title, timestamps = data\n",
    "\n",
    "        his_input_title = his_input_title.to(device)\n",
    "        pred_input_title = pred_input_title.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(pred_input_title, his_input_title)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        for i in range(outputs.size(0)):\n",
    "            pred_test.append(outputs[i].tolist())\n",
    "            labels_test.append(labels[i].tolist())\n",
    "\n",
    "        print(f\"Test iteration {iteration + 1}/{len(test_dataloader)}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "    print(\"Test loss:\", test_loss)\n",
    "\n",
    "print(pred_test)\n",
    "print(labels_test)\n",
    "\n",
    "from from_ebrec.evaluation import MetricEvaluator\n",
    "from from_ebrec.evaluation import AucScore, MrrScore, NdcgScore\n",
    "metrics = MetricEvaluator(\n",
    "    labels = labels_test,\n",
    "    predictions= pred_test,\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()\n",
    "print(metrics)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "number_to_print = 20\n",
    "print(\"Top %d predictions vs labels:\" % number_to_print)\n",
    "labels = dataset.df_test[\"labels\"].to_list()\n",
    "for i in range(number_to_print):\n",
    "    print(f\"Article {i}\")\n",
    "    for j in range(len(pred_test[i])):\n",
    "        print(f\"{pred_test[i][j]:.3f} vs {labels[i][j]:.3f}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Flatten the data for analysis\n",
    "predicted_probabilities = [prob for article in pred_test for prob in article]\n",
    "true_values = [val for article in labels[:len(pred_test)] for val in article]\n",
    "\n",
    "\n",
    "# Set a threshold (commonly 0.5) to classify probabilities as 0 or 1\n",
    "threshold = 0.5\n",
    "predicted_classes = [1 if p >= threshold else 0 for p in predicted_probabilities]\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_values, predicted_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate AUC and ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(true_values, predicted_classes)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve with AUC value explicitly highlighted\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label=\"Random Guess\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
