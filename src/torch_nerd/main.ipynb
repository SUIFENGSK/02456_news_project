{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T13:26:01.679641Z",
     "start_time": "2024-11-25T13:26:01.671293Z"
    }
   },
   "outputs": [],
   "source": [
    "BLACKHOLE = False\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "if BLACKHOLE:\n",
    "    workspace_path = os.path.expandvars('$BLACKHOLE')\n",
    "    sys.path.append(workspace_path+'/DeepLearning/02456_news_project/src')\n",
    "    DATAPATH = Path(workspace_path+\"/DeepLearning/ebnerd_data\").expanduser()\n",
    "else:\n",
    "    DATAPATH = Path(\"~/ebnerd_data\").expanduser()\n",
    "\n",
    "DATASET = \"ebnerd_demo\"\n",
    "# DATASET = \"ebnerd_small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages:\n",
    "- torch (PyTorch)\n",
    "- transformers (Huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1+cu124\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "\n",
    "# Check gpu availability\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Test:\n",
    "#print(torch.zeros(1).cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO HERFRA OG NED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T13:48:59.936309Z",
     "start_time": "2024-11-25T13:48:58.640561Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_handler import NewsDataset\n",
    "import from_ebrec._constants as cs\n",
    "\n",
    "SEED = 42\n",
    "HISTORY_SIZE = 20\n",
    "\n",
    "COLS = [\n",
    "    cs.DEFAULT_USER_COL,\n",
    "    cs.DEFAULT_IMPRESSION_ID_COL,\n",
    "    cs.DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    cs.DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    cs.DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    cs.DEFAULT_INVIEW_ARTICLES_COL,\n",
    "]\n",
    "\n",
    "FRACTION = 0.01\n",
    "\n",
    "# test\n",
    "dataset = NewsDataset(dataset_path=DATAPATH.joinpath(DATASET))\n",
    "\n",
    "dataset.setup_df(dataset_path = DATAPATH, datasplit = DATASET, history_size = HISTORY_SIZE, columns = COLS, fraction = FRACTION, seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as huggingface\n",
    "from from_ebrec._nlp import get_transformers_word_embeddings\n",
    "from from_ebrec._polars import concat_str_columns\n",
    "from from_ebrec._articles import convert_text2encoding_with_transformers\n",
    "from from_ebrec._articles import create_article_id_to_value_mapping\n",
    "\n",
    "\n",
    "df_articles = dataset.df_articles\n",
    "\n",
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "TEXT_COLUMNS_TO_USE = [cs.DEFAULT_SUBTITLE_COL, cs.DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = huggingface.AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = huggingface.AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import NRMSDataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors= dataset.df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column= cs.DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors= dataset.df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column= cs.DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSModel(\n",
      "  (word2vec_embedding): Embedding(250002, 768)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (news_encoder): NewsEncoder()\n",
      "  (user_encoder): UserEncoder()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from nrms import NRMSModel\n",
    "from hyperparameters import hparams_nrms\n",
    "\n",
    "\n",
    "hparams = hparams_nrms()\n",
    "hparams.history_size = HISTORY_SIZE\n",
    "\n",
    "# Ajust the hyperparameters to the model here:\n",
    "# E.g. head_num = 8, attention_hidden_dim = 512, etc.\n",
    "\n",
    "model = NRMSModel(hparams=hparams, word2vec_embedding=word2vec_embedding, seed = SEED)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/5:   0%|          | 0/7 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [32, 30] but got: [32, 5].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(pred_input_title, his_input_title)  \n\u001b[0;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\andre\\Documents\\Dev\\deep_learning\\02456_news_project\\src\\torch_nerd\\nrms.py:73\u001b[0m, in \u001b[0;36mNRMSModel.forward\u001b[1;34m(self, clicked_titles, candidate_titles)\u001b[0m\n\u001b[0;32m     70\u001b[0m news_rep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnews_encoder\u001b[38;5;241m.\u001b[39mforward(candidate_titles)\n\u001b[0;32m     71\u001b[0m user_rep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_encoder\u001b[38;5;241m.\u001b[39mforward(clicked_titles)\n\u001b[1;32m---> 73\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(news_rep, user_rep)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39msoftmax(scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [32, 30] but got: [32, 5]."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm  # for progress bars\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Adjust based on your task\n",
    "optimizer = optim.Adam(model.parameters(), lr=hparams_nrms.learning_rate)\n",
    "\n",
    "# Move model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "train_loss_history, val_loss_history = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}/{EPOCHS}\"):\n",
    "        # Unpacking of batch\n",
    "        (inputs, labels) = batch\n",
    "        his_input_title, pred_input_title = inputs\n",
    "\n",
    "        # Move data to device\n",
    "        his_input_title = his_input_title.to(device)\n",
    "        pred_input_title = pred_input_title.to(device)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pred_input_title, his_input_title)  \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=f\"Validation Epoch {epoch + 1}/{EPOCHS}\"):\n",
    "            (inputs, labels) = batch\n",
    "            his_input_title, pred_input_title = inputs\n",
    "\n",
    "            his_input_title = his_input_title.to(device)\n",
    "            pred_input_title = pred_input_title.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(pred_input_title, his_input_title)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    val_loss /= len(val_dataloader)\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
